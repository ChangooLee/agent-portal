# LangChain Response Metadata ê°€ì´ë“œ

## ê°œìš”

LangChainì˜ `AIMessage` ê°ì²´ëŠ” `response_metadata` ì†ì„±ì„ í†µí•´ LLM ì‘ë‹µê³¼ í•¨ê»˜ ë‹¤ì–‘í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ë©”íƒ€ë°ì´í„°ëŠ” **provider(OpenAI, Anthropic ë“±)ì— ë”°ë¼ ë‹¤ë¥¸ êµ¬ì¡°**ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

## Providerë³„ Response Metadata êµ¬ì¡°

### 1. OpenAI ê¸°ë°˜ (ChatOpenAI)

OpenAI APIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° (`ChatOpenAI`), `response_metadata`ëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

```python
{
    "token_usage": {
        "completion_tokens": 88,
        "prompt_tokens": 16,
        "total_tokens": 104,
        "completion_tokens_details": {
            "accepted_prediction_tokens": 0,
            "audio_tokens": 0,
            "reasoning_tokens": 0,
            "rejected_prediction_tokens": 0
        },
        "prompt_tokens_details": {
            "audio_tokens": 0,
            "cached_tokens": 0
        }
    },
    "model_name": "gpt-4o-mini-2024-07-18",
    "system_fingerprint": "fp_34a54ae93c",
    "id": "chatcmpl-ByN1Qkvqb5fAGKKzXXxZ3rBlnqkWs",
    "service_tier": "default",
    "finish_reason": "stop",  # ë˜ëŠ” "tool_call", "length", "content_filter", "function_call", "max_tokens"
    "logprobs": None
}
```

**ì£¼ìš” í•„ë“œ**:
- `token_usage`: í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
  - `completion_tokens`: ì‘ë‹µì— ì‚¬ìš©ëœ í† í° ìˆ˜
  - `prompt_tokens`: í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©ëœ í† í° ìˆ˜
  - `total_tokens`: ì „ì²´ í† í° ìˆ˜
- `model_name`: ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„
- `finish_reason`: ì‘ë‹µ ì¢…ë£Œ ì´ìœ 
  - `"stop"`: ì •ìƒ ì¢…ë£Œ
  - `"tool_call"`: ë„êµ¬ í˜¸ì¶œ í•„ìš”
  - `"length"`: ê¸¸ì´ ì œí•œ ë„ë‹¬
  - `"content_filter"`: ì½˜í…ì¸  í•„í„°ë§
  - `"function_call"`: í•¨ìˆ˜ í˜¸ì¶œ
  - `"max_tokens"`: ìµœëŒ€ í† í° ë„ë‹¬
- `system_fingerprint`: ì‹œìŠ¤í…œ ì§€ë¬¸
- `id`: ì‘ë‹µ ID
- `service_tier`: ì„œë¹„ìŠ¤ í‹°ì–´
- `logprobs`: ë¡œê·¸ í™•ë¥  ì •ë³´ (ì„ íƒì )

### 2. Anthropic ê¸°ë°˜

Anthropic APIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, `response_metadata`ëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

```python
{
    "model": "claude-3-5-sonnet-20241022",
    "usage": {
        "input_tokens": 100,
        "output_tokens": 200
    },
    "stop_reason": "end_turn"  # ë˜ëŠ” "max_tokens", "stop_sequence"
}
```

**ì£¼ìš” í•„ë“œ**:
- `model`: ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„
- `usage`: í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
  - `input_tokens`: ì…ë ¥ í† í° ìˆ˜
  - `output_tokens`: ì¶œë ¥ í† í° ìˆ˜
- `stop_reason`: ì‘ë‹µ ì¢…ë£Œ ì´ìœ 
  - `"end_turn"`: ì •ìƒ ì¢…ë£Œ
  - `"max_tokens"`: ìµœëŒ€ í† í° ë„ë‹¬
  - `"stop_sequence"`: ì¤‘ì§€ ì‹œí€€ìŠ¤ ë„ë‹¬

## ì½”ë“œë² ì´ìŠ¤ì—ì„œì˜ ì‚¬ìš© ì˜ˆì‹œ

### langflow ì˜ˆì‹œ

```python
# langflow/src/backend/base/langflow/base/models/model.py
if message.response_metadata:
    response_metadata = message.response_metadata
    
    # OpenAI í˜•ì‹ í™•ì¸
    openai_keys = ["token_usage", "model_name", "finish_reason"]
    inner_openai_keys = ["completion_tokens", "prompt_tokens", "total_tokens"]
    
    if all(key in response_metadata for key in openai_keys):
        token_usage = response_metadata["token_usage"]
        finish_reason = response_metadata["finish_reason"]
    
    # Anthropic í˜•ì‹ í™•ì¸
    anthropic_keys = ["model", "usage", "stop_reason"]
    inner_anthropic_keys = ["input_tokens", "output_tokens"]
    
    if all(key in response_metadata for key in anthropic_keys):
        usage = response_metadata["usage"]
        stop_reason = response_metadata["stop_reason"]
```

### ìš°ë¦¬ í”„ë¡œì íŠ¸ì—ì„œì˜ ì‚¬ìš©

```python
# backend/app/agents/common/base_single_agent.py
response_metadata = getattr(response, "response_metadata", {})

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
token_usage = response_metadata.get("token_usage", {}) or {}

# finish_reason ì¶”ì¶œ (OpenAI í˜•ì‹)
finish_reason = response_metadata.get("finish_reason", "")

# finish_reasonì´ ì—†ìœ¼ë©´ tool_calls ìœ ë¬´ë¡œ ì¶”ë¡ 
if not finish_reason:
    if hasattr(response, "tool_calls") and response.tool_calls:
        finish_reason = "tool_call"
    else:
        finish_reason = "stop"
```

## ì£¼ì˜ì‚¬í•­

1. **Providerì— ë”°ë¼ ë‹¤ë¥¸ í•„ë“œëª…**: OpenAIëŠ” `finish_reason`, Anthropicì€ `stop_reason`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
2. **í† í° ì‚¬ìš©ëŸ‰ í•„ë“œëª…**: OpenAIëŠ” `token_usage`, Anthropicì€ `usage`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. **í•­ìƒ ì¡´ì¬í•˜ì§€ ì•ŠìŒ**: `response_metadata`ëŠ” í•­ìƒ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, `getattr()` ë˜ëŠ” `.get()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.
4. **finish_reasonì´ ì—†ëŠ” ê²½ìš°**: ì¼ë¶€ providerë‚˜ ëª¨ë¸ì—ì„œëŠ” `finish_reason`ì´ ì œê³µë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° `tool_calls` ìœ ë¬´ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ìš°ë¦¬ í”„ë¡œì íŠ¸ì˜ êµ¬í˜„

### Provider ë…ë¦½ì  finish_reason ì¶”ì¶œ

`backend/app/agents/common/base_single_agent.py`ì—ì„œ Providerì— ê´€ê³„ì—†ì´ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤:

```python
def _extract_finish_reason(self, response: Any, response_metadata: Dict[str, Any]) -> str:
    """
    Providerì— ê´€ê³„ì—†ì´ finish_reason ì¶”ì¶œ
    - OpenAI: finish_reason ì§ì ‘ ì‚¬ìš©
    - Anthropic: stop_reasonì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    - ì—†ìœ¼ë©´: tool_calls ìœ ë¬´ë¡œ ì¶”ë¡ 
    """
    # 1. OpenAI í˜•ì‹ í™•ì¸
    finish_reason = response_metadata.get("finish_reason", "")
    
    # 2. Anthropic í˜•ì‹ í™•ì¸ ë° ë³€í™˜
    if not finish_reason:
        stop_reason = response_metadata.get("stop_reason", "")
        if stop_reason:
            stop_reason_map = {
                "end_turn": "stop",
                "max_tokens": "length",
                "stop_sequence": "stop"
            }
            finish_reason = stop_reason_map.get(stop_reason, "stop")
    
    # 3. ì¶”ë¡  (tool_calls ìœ ë¬´)
    if not finish_reason:
        if hasattr(response, "tool_calls") and response.tool_calls:
            finish_reason = "tool_call"
        else:
            finish_reason = "stop"
    
    return finish_reason
```

### ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ ë³€í™˜

```python
def _get_finish_reason_message(self, finish_reason: str) -> str:
    """finish_reasonì„ ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜"""
    finish_reason_messages = {
        "stop": "âœ… ì‘ë‹µ ì™„ë£Œ",
        "tool_call": "ğŸ”§ ë„êµ¬ í˜¸ì¶œ í•„ìš”",
        "length": "âš ï¸ ê¸¸ì´ ì œí•œ ë„ë‹¬",
        "content_filter": "âš ï¸ ì½˜í…ì¸  í•„í„°ë§",
        "function_call": "ğŸ”§ í•¨ìˆ˜ í˜¸ì¶œ",
        "max_tokens": "âš ï¸ ìµœëŒ€ í† í° ë„ë‹¬"
    }
    return finish_reason_messages.get(finish_reason, f"â³ ì²˜ë¦¬ ì¤‘ ({finish_reason})")
```

### í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œ

ëª¨ë“  ì—ì´ì „íŠ¸ í™”ë©´ì—ì„œ ë°±ì—”ë“œì—ì„œ ë³€í™˜ëœ ì¹œí™”ì  ë©”ì‹œì§€ë¥¼ ê·¸ëŒ€ë¡œ í‘œì‹œí•©ë‹ˆë‹¤:

```typescript
case 'progress':
    // finish_reasonì´ ìˆìœ¼ë©´ ë°±ì—”ë“œì—ì„œ ì´ë¯¸ ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜ë˜ì–´ ìˆìŒ
    const progressMsg = data.message || data.content || 'ë¶„ì„ ì§„í–‰ ì¤‘...';
    currentToolCall = transformProgressMessage(progressMsg);
    break;
```

## ì°¸ê³  ìë£Œ

- [LangChain AIMessage ë¬¸ì„œ](https://python.langchain.com/docs/modules/model_io/chat/messages/)
- [OpenAI API ì‘ë‹µ êµ¬ì¡°](https://platform.openai.com/docs/api-reference/chat/object)
- [Anthropic API ì‘ë‹µ êµ¬ì¡°](https://docs.anthropic.com/claude/reference/messages-post)

