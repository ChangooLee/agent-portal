"""
Base Single Agent - ë²”ìš© MCP ê¸°ë°˜ Single Agent

Single Agent íŒ¨í„´ì„ ì¼ë°˜í™”í•œ ë² ì´ìŠ¤ í´ë˜ìŠ¤.
ê° ë„ë©”ì¸ë³„ ì—ì´ì „íŠ¸ê°€ ìƒì†í•˜ì—¬ ì‚¬ìš©.
"""

import json
import logging
import time
import os
from abc import ABC, abstractmethod
from contextlib import contextmanager
from datetime import datetime
from typing import Dict, Any, List, Optional, AsyncGenerator, Type

from pydantic import BaseModel, Field, create_model
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import BaseTool, StructuredTool
from langchain_openai import ChatOpenAI

from app.agents.common.mcp_client_base import MCPClientBase, MCPTool, create_mcp_client

logger = logging.getLogger(__name__)

# OTEL tracing metrics í•¨ìˆ˜ë“¤ import (LLM/ë„êµ¬ í˜¸ì¶œ ë¡œê¹…ìš©)
try:
    from app.agents.dart_agent.metrics import (
        start_llm_call_span,
        start_tool_call_span,
        get_trace_headers,
        inject_context_to_carrier,
        set_active_service
    )
    logger.info("OTEL tracing metrics imported successfully for base_single_agent")
except ImportError as e:
    logger.warning(f"OTEL tracing metrics not available, using no-op: {e}")
    start_llm_call_span = None
    start_tool_call_span = None
    get_trace_headers = lambda: {}
    inject_context_to_carrier = lambda x: None
    set_active_service = lambda x: None


# OTEL íŠ¸ë ˆì´ì‹± í—¬í¼
_service_tracers = {}

def _get_tracer_for_agent(service_name: str):
    """ì—ì´ì „íŠ¸ë³„ tracer íšë“"""
    global _service_tracers
    
    if service_name in _service_tracers:
        return _service_tracers[service_name]
    
    try:
        from app.telemetry.otel import get_tracer_for_service
        tracer = get_tracer_for_service(service_name, f"{service_name}-tracer")
        _service_tracers[service_name] = tracer
        return tracer
    except Exception as e:
        logging.getLogger(__name__).warning(f"Failed to get tracer for {service_name}: {e}")
        return None


@contextmanager
def _start_agent_span(service_name: str, span_name: str, attributes: Dict[str, Any] = None):
    """ì—ì´ì „íŠ¸ìš© span ì‹œì‘"""
    tracer = _get_tracer_for_agent(service_name)
    
    if tracer is None:
        yield None
        return
    
    span_attrs = {
        "service.name": service_name,
        "agent.type": service_name,
    }
    if attributes:
        span_attrs.update(attributes)
    
    try:
        with tracer.start_as_current_span(span_name, attributes=span_attrs) as span:
            yield span
    except Exception as e:
        logging.getLogger(__name__).warning(f"Failed to create span: {e}")
        yield None


def _get_traceparent_headers() -> Dict[str, str]:
    """í˜„ì¬ contextì—ì„œ traceparent í—¤ë” ìƒì„±"""
    try:
        from opentelemetry.propagate import inject
        carrier = {}
        inject(carrier)
        return carrier
    except Exception:
        return {}


def _get_current_span_carrier() -> Dict[str, str]:
    """í˜„ì¬ span carrier ë°˜í™˜"""
    carrier = {}
    try:
        inject_context_to_carrier(carrier)
    except Exception:
        pass
    return carrier


logger = logging.getLogger(__name__)


def _create_args_schema(input_schema: Dict[str, Any], tool_name: str) -> Type[BaseModel]:
    """MCP input_schemaì—ì„œ Pydantic ëª¨ë¸ ë™ì  ìƒì„±"""
    properties = input_schema.get("properties", {})
    required = input_schema.get("required", [])
    
    fields = {}
    for prop_name, prop_info in properties.items():
        if prop_name == "ctx":
            continue
        
        prop_type = prop_info.get("type", "string")
        description = prop_info.get("description", "")
        default = prop_info.get("default", ...)
        
        type_map = {
            "string": str,
            "integer": int,
            "number": float,
            "boolean": bool,
            "array": list,
            "object": dict,
        }
        python_type = type_map.get(prop_type, str)
        
        if prop_name in required:
            fields[prop_name] = (python_type, Field(description=description))
        else:
            if default is ...:
                default = None
            fields[prop_name] = (Optional[python_type], Field(default=default, description=description))
    
    model_name = f"{tool_name.replace('-', '_').title().replace('_', '')}Args"
    return create_model(model_name, **fields)


class BaseSingleAgent(ABC):
    """
    ë²”ìš© MCP ê¸°ë°˜ Single Agent ë² ì´ìŠ¤ í´ë˜ìŠ¤
    
    Claude Opus 4.5ë¥¼ ì‚¬ìš©í•˜ì—¬ ReAct íŒ¨í„´ìœ¼ë¡œ ë™ì‘.
    ê° ë„ë©”ì¸ë³„ ì—ì´ì „íŠ¸ê°€ ìƒì†í•˜ì—¬ ì‚¬ìš©.
    """
    
    # ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
    MCP_SERVER_NAME: str = ""  # MCP ì„œë²„ ì´ë¦„
    SERVICE_NAME: str = "agent-base"  # OTEL ì„œë¹„ìŠ¤ ì´ë¦„
    AGENT_DISPLAY_NAME: str = "Base Agent"  # í™”ë©´ í‘œì‹œëª…
    
    def __init__(self, model: str = "claude-opus-4.5"):
        self.model_name = model
        self.max_iterations = 30
        self.llm = None
        self.tools: List[BaseTool] = []
        self.mcp_client: Optional[MCPClientBase] = None
        self._initialized = False
    
    async def _ensure_mcp_initialized(self):
        """MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if self.mcp_client is not None:
            return
        
        # MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.mcp_client = await create_mcp_client(
            server_name=self.MCP_SERVER_NAME,
            service_name=self.SERVICE_NAME
        )
        
        # MCP ë„êµ¬ë“¤ì„ LangChain Toolë¡œ ë˜í•‘
        mcp_tools = self.mcp_client.get_tools()
        self.tools = await self._wrap_mcp_tools(mcp_tools)
        logger.info(f"[{self.SERVICE_NAME}] MCP ì´ˆê¸°í™” ì™„ë£Œ: {len(self.tools)}ê°œ ë„êµ¬ ë¡œë“œ")
    
    def _create_llm(self) -> ChatOpenAI:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (traceparent í—¤ë” í¬í•¨)"""
        litellm_api_key = os.environ.get("LITELLM_MASTER_KEY", "sk-1234")
        trace_headers = _get_traceparent_headers()
        
        logger.debug(f"[{self.SERVICE_NAME}] LLM ìƒì„± (traceparent: {trace_headers.get('traceparent', 'none')})")
        
        return ChatOpenAI(
            model=self.model_name,
            base_url="http://litellm:4000/v1",
            api_key=litellm_api_key,
            default_headers=trace_headers,  # traceparent ì „ë‹¬
            temperature=0.1,
            max_tokens=16384,
            timeout=600,
        )
    
    async def _wrap_mcp_tools(self, mcp_tools: List[MCPTool]) -> List[BaseTool]:
        """MCP ë„êµ¬ë“¤ì„ LangChain Toolë¡œ ë˜í•‘"""
        langchain_tools = []
        
        for mcp_tool in mcp_tools:
            tool_name = mcp_tool.name
            input_schema = mcp_tool.input_schema
            
            # args_schema ë™ì  ìƒì„±
            try:
                args_schema = _create_args_schema(input_schema, tool_name)
            except Exception as e:
                logger.warning(f"ë„êµ¬ {tool_name}ì˜ args_schema ìƒì„± ì‹¤íŒ¨: {e}")
                args_schema = None
            
            # í´ë¡œì €ë¥¼ ìœ„í•œ ìº¡ì²˜
            def make_coroutine(captured_tool_name, captured_mcp_client):
                async def coro(**kwargs) -> str:
                    kwargs.pop("ctx", None)
                    try:
                        result = await captured_mcp_client.call_tool(captured_tool_name, kwargs)
                        if result.error:
                            return json.dumps({"error": result.error}, ensure_ascii=False)
                        return result.result if isinstance(result.result, str) else json.dumps(result.result, ensure_ascii=False)
                    except Exception as e:
                        return json.dumps({"error": str(e)}, ensure_ascii=False)
                return coro
            
            # StructuredTool ìƒì„±
            tool_kwargs = {
                "func": lambda **kwargs: None,
                "coroutine": make_coroutine(tool_name, self.mcp_client),
                "name": tool_name,
                "description": mcp_tool.description or f"MCP tool: {tool_name}",
            }
            
            if args_schema:
                tool_kwargs["args_schema"] = args_schema
            
            lc_tool = StructuredTool.from_function(**tool_kwargs)
            langchain_tools.append(lc_tool)
        
        return langchain_tools
    
    @abstractmethod
    def _build_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± - ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        pass
    
    def _get_tool_display_name(self, tool_name: str) -> str:
        """ë„êµ¬ í‘œì‹œëª… ë°˜í™˜ - ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥"""
        return tool_name
    
    def _extract_finish_reason(self, response: Any, response_metadata: Dict[str, Any]) -> str:
        """
        Providerì— ê´€ê³„ì—†ì´ finish_reason ì¶”ì¶œ
        
        Args:
            response: LangChain AIMessage ê°ì²´
            response_metadata: response_metadata ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ì •ê·œí™”ëœ finish_reason ë¬¸ìì—´
        """
        if not isinstance(response_metadata, dict):
            response_metadata = {}
        
        # 1. OpenAI í˜•ì‹: finish_reason í™•ì¸
        finish_reason = response_metadata.get("finish_reason", "")
        
        # 2. Anthropic í˜•ì‹: stop_reason í™•ì¸ (finish_reasonì´ ì—†ì„ ë•Œ)
        if not finish_reason:
            stop_reason = response_metadata.get("stop_reason", "")
            if stop_reason:
                # Anthropicì˜ stop_reasonì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                stop_reason_map = {
                    "end_turn": "stop",
                    "max_tokens": "length",
                    "stop_sequence": "stop"
                }
                finish_reason = stop_reason_map.get(stop_reason, "stop")
        
        # 3. response_metadataì— ì—†ìœ¼ë©´ tool_calls ìœ ë¬´ë¡œ ì¶”ë¡ 
        if not finish_reason:
            if hasattr(response, "tool_calls") and response.tool_calls:
                finish_reason = "tool_call"
            else:
                finish_reason = "stop"
        
        return finish_reason
    
    def _get_finish_reason_message(self, finish_reason: str) -> str:
        """
        finish_reasonì„ ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜
        
        Args:
            finish_reason: ì •ê·œí™”ëœ finish_reason ë¬¸ìì—´
            
        Returns:
            ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€
        """
        finish_reason_messages = {
            "stop": "âœ… ì‘ë‹µ ì™„ë£Œ",
            "tool_call": "ğŸ”§ ë„êµ¬ í˜¸ì¶œ í•„ìš”",
            "tool_calls": "ğŸ”§ ë„êµ¬ í˜¸ì¶œ í•„ìš”",
            "length": "âš ï¸ ê¸¸ì´ ì œí•œ ë„ë‹¬",
            "content_filter": "âš ï¸ ì½˜í…ì¸  í•„í„°ë§",
            "function_call": "ğŸ”§ í•¨ìˆ˜ í˜¸ì¶œ",
            "max_tokens": "âš ï¸ ìµœëŒ€ í† í° ë„ë‹¬"
        }
        return finish_reason_messages.get(finish_reason, f"â³ ì²˜ë¦¬ ì¤‘ ({finish_reason})")
    
    async def analyze_stream(
        self,
        question: str,
        session_id: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹¤í–‰
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡
            
        Yields:
            SSE ì´ë²¤íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        logger.info(f"[{self.SERVICE_NAME}] Starting analysis: {question[:50]}...")
        
        # OTEL tracingì— ì„œë¹„ìŠ¤ ì´ë¦„ ì„¤ì • (LLM/ë„êµ¬ í˜¸ì¶œ spanì´ ì˜¬ë°”ë¥¸ service name ì‚¬ìš©)
        if set_active_service is not None:
            set_active_service(self.SERVICE_NAME)
            logger.debug(f"[{self.SERVICE_NAME}] Set active service for OTEL tracing: {self.SERVICE_NAME}")
        
        # OTEL Root Span ìƒì„±
        with _start_agent_span(
            self.SERVICE_NAME,
            f"gen_ai.session.{self.SERVICE_NAME}",
            {
                "gen_ai.agent.id": self.SERVICE_NAME,
                "gen_ai.agent.name": self.AGENT_DISPLAY_NAME,
                "gen_ai.request.model": self.model_name,
                "gen_ai.user_query": question[:200],
                "session.id": session_id or "",
            }
        ) as root_span:
            try:
                # MCP ì´ˆê¸°í™”
                await self._ensure_mcp_initialized()
                
                # Start ì´ë²¤íŠ¸
                yield {
                    "event": "start",
                    "message": f"{self.AGENT_DISPLAY_NAME}ê°€ ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤...",
                    "agent": self.SERVICE_NAME
                }
                
                # LLM ìƒì„± (root span ë‚´ì—ì„œ ìƒì„±í•˜ì—¬ traceparent í¬í•¨)
                self.llm = self._create_llm()
                
                # í˜„ì¬ spanì˜ contextë¥¼ carrierì— ì €ì¥ (í•˜ìœ„ í˜¸ì¶œì— ì „ë‹¬ìš©)
                parent_carrier = _get_current_span_carrier()
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
                system_prompt = self._build_system_prompt()
                
                # ë©”ì‹œì§€ êµ¬ì„±
                messages = [SystemMessage(content=system_prompt)]
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
                if conversation_history:
                    for msg in conversation_history[-10:]:  # ìµœê·¼ 10ê°œ
                        role = msg.get("role", "")
                        content = msg.get("content", "")
                        if role == "user":
                            messages.append(HumanMessage(content=content))
                        elif role == "assistant":
                            messages.append(AIMessage(content=content))
                
                # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
                messages.append(HumanMessage(content=question))
                
                # LLM + Tools ë°”ì¸ë”©
                llm_with_tools = self.llm.bind_tools(self.tools)
                
                # ReAct ë£¨í”„
                iteration = 0
                final_response = ""
                
                while iteration < self.max_iterations:
                    iteration += 1
                    logger.debug(f"[{self.SERVICE_NAME}] Iteration {iteration}")
                    
                    # Iteration span
                    with _start_agent_span(
                        self.SERVICE_NAME,
                        f"gen_ai.iteration.{iteration}",
                        {"iteration": iteration}
                    ):
                        yield {
                            "event": "iteration",
                            "iteration": iteration,
                            "max_iterations": self.max_iterations
                        }
                        
                        # LLM í˜¸ì¶œ (spanìœ¼ë¡œ ê¸°ë¡)
                        # ë©”ì‹œì§€ë¥¼ dictë¡œ ë³€í™˜ (OTEL tracing í˜•ì‹)
                        messages_dict = []
                        for msg in messages:
                            if isinstance(msg, SystemMessage):
                                messages_dict.append({"role": "system", "content": msg.content})
                            elif isinstance(msg, HumanMessage):
                                messages_dict.append({"role": "user", "content": msg.content})
                            elif isinstance(msg, AIMessage):
                                messages_dict.append({"role": "assistant", "content": msg.content})
                            elif isinstance(msg, ToolMessage):
                                messages_dict.append({"role": "tool", "content": msg.content})
                            else:
                                messages_dict.append({"role": "user", "content": str(msg)})
                        
                        logger.info(f"[{self.SERVICE_NAME}] About to call LLM, start_llm_call_span is None: {start_llm_call_span is None}")
                        if start_llm_call_span is not None:
                            logger.debug(f"[{self.SERVICE_NAME}] Using start_llm_call_span for LLM call")
                            with start_llm_call_span(
                                self.SERVICE_NAME,
                                self.model_name,
                                messages_dict,
                                parent_carrier
                            ) as (llm_span, record_llm):
                                try:
                                    logger.info(f"[{self.SERVICE_NAME}] Calling LLM with tools...")
                                    response = await llm_with_tools.ainvoke(messages)
                                    logger.info(f"[{self.SERVICE_NAME}] LLM response received, type: {type(response)}")
                                    # LLM ì‘ë‹µ ê¸°ë¡ (LangChain AIMessageë¥¼ dictë¡œ ë³€í™˜)
                                    response_metadata = getattr(response, "response_metadata", {})
                                    logger.info(f"[{self.SERVICE_NAME}] Got response_metadata: {response_metadata is not None}, type: {type(response_metadata)}")
                                    token_usage = response_metadata.get("token_usage", {}) or {}
                                    
                                    response_dict = {
                                        "model": self.model_name,
                                        "choices": [{
                                            "message": {
                                                "content": response.content if hasattr(response, "content") else "",
                                                "tool_calls": [
                                                    {
                                                        "id": tc.get("id", ""),
                                                        "name": tc.get("name", ""),
                                                        "args": tc.get("args", {})
                                                    }
                                                    for tc in (response.tool_calls if hasattr(response, "tool_calls") else [])
                                                ]
                                            },
                                            "finish_reason": response_metadata.get("finish_reason", "")
                                        }],
                                        "usage": {
                                            "prompt_tokens": token_usage.get("prompt_tokens", 0),
                                            "completion_tokens": token_usage.get("completion_tokens", 0),
                                            "total_tokens": token_usage.get("total_tokens", 0)
                                        }
                                    }
                                    record_llm(response_dict)
                                    
                                    # finish_reasonì„ SSE ì´ë²¤íŠ¸ë¡œ ì „ë‹¬ (Provider ë…ë¦½ì )
                                    logger.info(f"[{self.SERVICE_NAME}] response_metadata full: {response_metadata}")
                                    logger.info(f"[{self.SERVICE_NAME}] response_metadata type: {type(response_metadata)}")
                                    
                                    # Providerì— ê´€ê³„ì—†ì´ finish_reason ì¶”ì¶œ
                                    finish_reason = self._extract_finish_reason(response, response_metadata)
                                    
                                    logger.info(f"[{self.SERVICE_NAME}] LLM response finish_reason: {finish_reason}, response_metadata keys: {list(response_metadata.keys()) if isinstance(response_metadata, dict) else 'N/A'}")
                                    
                                    # LLM ì‘ë‹µ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ í‘œì‹œ (í•„í„°ë§ ì—†ì´)
                                    llm_response_content = response.content if hasattr(response, "content") else ""
                                    logger.info(f"[{self.SERVICE_NAME}] Yielding finish_reason event: finish_reason={finish_reason}, content_length={len(llm_response_content)}")
                                    yield {
                                        "event": "progress",
                                        "message": llm_response_content if llm_response_content else f"finish_reason: {finish_reason}",
                                        "finish_reason": finish_reason
                                    }
                                except Exception as e:
                                    logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                                    if llm_span and hasattr(llm_span, "set_attribute"):
                                        llm_span.set_attribute("error", str(e))
                                    raise
                        else:
                            # start_llm_call_spanì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í˜¸ì¶œ
                            logger.info(f"[{self.SERVICE_NAME}] Calling LLM without span...")
                            response = await llm_with_tools.ainvoke(messages)
                            logger.info(f"[{self.SERVICE_NAME}] LLM response received (no span), type: {type(response)}")
                            
                            # finish_reason ì¶”ì¶œ ë° ì „ë‹¬ (Provider ë…ë¦½ì )
                            response_metadata = getattr(response, "response_metadata", {})
                            logger.info(f"[{self.SERVICE_NAME}] Got response_metadata (no span): {response_metadata is not None}, type: {type(response_metadata)}")
                            
                            # Providerì— ê´€ê³„ì—†ì´ finish_reason ì¶”ì¶œ
                            finish_reason = self._extract_finish_reason(response, response_metadata)
                            
                            logger.info(f"[{self.SERVICE_NAME}] LLM response finish_reason (no span): {finish_reason}, response_metadata keys: {list(response_metadata.keys()) if isinstance(response_metadata, dict) else 'N/A'}")
                            
                            # LLM ì‘ë‹µ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ í‘œì‹œ (í•„í„°ë§ ì—†ì´)
                            llm_response_content = response.content if hasattr(response, "content") else ""
                            logger.info(f"[{self.SERVICE_NAME}] Yielding finish_reason event (no span): finish_reason={finish_reason}, content_length={len(llm_response_content)}")
                            yield {
                                "event": "progress",
                                "message": llm_response_content if llm_response_content else f"finish_reason: {finish_reason}",
                                "finish_reason": finish_reason
                            }
                        
                        # ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
                        if response.tool_calls:
                            messages.append(response)
                            
                            for tool_call in response.tool_calls:
                                tool_name = tool_call["name"]
                                tool_args = tool_call["args"]
                                tool_call_id = tool_call.get("id", f"call_{iteration}")
                                
                                display_name = self._get_tool_display_name(tool_name)
                                
                                yield {
                                    "event": "tool_start",
                                    "tool": tool_name,
                                    "display_name": display_name,
                                    "args": tool_args
                                }
                                
                                # Tool execution span (OTEL tracing ì‚¬ìš©)
                                if start_tool_call_span is not None:
                                    logger.debug(f"[{self.SERVICE_NAME}] Using start_tool_call_span for tool: {tool_name}")
                                    with start_tool_call_span(
                                        tool_name,
                                        tool_args,
                                        parent_carrier
                                    ) as (tool_span, record_tool):
                                        # ë„êµ¬ ì‹¤í–‰
                                        tool_result = None
                                        tool_start_time = time.time()
                                        for tool in self.tools:
                                            if tool.name == tool_name:
                                                try:
                                                    tool_result = await tool.ainvoke(tool_args)
                                                except Exception as e:
                                                    tool_result = json.dumps({"error": str(e)}, ensure_ascii=False)
                                                break
                                        
                                        if tool_result is None:
                                            tool_result = json.dumps({"error": f"Unknown tool: {tool_name}"}, ensure_ascii=False)
                                        
                                        # ë„êµ¬ ê²°ê³¼ ê¸°ë¡
                                        tool_latency_ms = (time.time() - tool_start_time) * 1000
                                        record_tool(tool_result, tool_latency_ms)
                                else:
                                    # start_tool_call_spanì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì‹¤í–‰
                                    tool_result = None
                                    for tool in self.tools:
                                        if tool.name == tool_name:
                                            try:
                                                tool_result = await tool.ainvoke(tool_args)
                                            except Exception as e:
                                                tool_result = json.dumps({"error": str(e)}, ensure_ascii=False)
                                            break
                                    
                                    if tool_result is None:
                                        tool_result = json.dumps({"error": f"Unknown tool: {tool_name}"}, ensure_ascii=False)
                                
                                yield {
                                    "event": "tool_result",
                                    "tool": tool_name,
                                    "display_name": display_name,
                                    "success": "error" not in str(tool_result).lower()
                                }
                                
                                # ToolMessage ì¶”ê°€
                                messages.append(ToolMessage(
                                    content=tool_result if isinstance(tool_result, str) else json.dumps(tool_result, ensure_ascii=False),
                                    tool_call_id=tool_call_id
                                ))
                        else:
                            # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ìµœì¢… ì‘ë‹µ
                            final_response = response.content
                            break
                
                # ìµœì¢… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                if final_response:
                    yield {
                        "event": "content",
                        "content": final_response
                    }
                
                elapsed_time = time.time() - start_time
                
                # Spanì— ê²°ê³¼ ê¸°ë¡
                if root_span:
                    root_span.set_attribute("gen_ai.response.latency_ms", elapsed_time * 1000)
                    root_span.set_attribute("gen_ai.response.iterations", iteration)
                
                yield {
                    "event": "complete",
                    "elapsed_time": elapsed_time,
                    "iterations": iteration
                }
                
                logger.info(f"[{self.SERVICE_NAME}] Analysis completed in {elapsed_time:.2f}s ({iteration} iterations)")
                
            except Exception as e:
                logger.error(f"[{self.SERVICE_NAME}] Analysis failed: {e}", exc_info=True)
                if root_span:
                    root_span.set_attribute("error", True)
                    root_span.set_attribute("error.message", str(e))
                yield {
                    "event": "error",
                    "message": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                }

